{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/welcomeglory/Python/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HPEv8UodwER",
        "outputId": "e127b40a-adc9-4802-ee71-9887e1781207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from Korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (4.66.4)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.31.0)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2024.7.4)\n",
            "Collecting konlpy\n",
            "  Using cached konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Using cached JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "Using cached JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n",
            "Collecting tensorflow==2.15.0\n",
            "  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Using cached wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.63 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install Korpora\n",
        "!pip install konlpy\n",
        "!pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-Wq9dU3pe8GV",
        "outputId": "c5470d41-d86f-41c5-aaa1-97ca607d39bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Korpora import Korpora\n",
        "from tqdm import tqdm\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWSJl7uegIGK",
        "outputId": "3abb7da4-08c8-4b1c-a91c-2eccc0cabfd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n",
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n"
          ]
        }
      ],
      "source": [
        "Korpora.fetch(\"nsmc\")\n",
        "corpus = Korpora.load(\"nsmc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZYbhHr6Bmn6m"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model class with Dropout layers\n",
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, sequence_length, vocab_size, embedding_dim, hidden_size, num_of_class, dropout_rate=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length)\n",
        "        self.lstm_layer = tf.keras.layers.LSTM(hidden_size, return_sequences=False, stateful=False, recurrent_initializer='glorot_uniform')\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.hidden_layer1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.hidden_layer2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        if num_of_class == 2:\n",
        "            self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')  # 이진 분류\n",
        "        else:\n",
        "            self.output_layer = tf.keras.layers.Dense(num_of_class, activation='softmax')  # 다중 클래스 분류\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        embedded_input = self.embedding_layer(x)\n",
        "        features = self.lstm_layer(embedded_input)\n",
        "        features = self.dropout1(features, training=training)\n",
        "        hid1 = self.hidden_layer1(features)\n",
        "        hid1 = self.dropout2(hid1, training=training)\n",
        "        hid2 = self.hidden_layer2(hid1)\n",
        "        hid2 = self.dropout3(hid2, training=training)\n",
        "        logits = self.output_layer(hid2)\n",
        "        return logits\n",
        "\n",
        "# Loss function\n",
        "@tf.function\n",
        "def cross_entropy_loss(logits, y, num_of_class):\n",
        "    if num_of_class == 2:  # 이진 분류\n",
        "        loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y, logits, from_logits=True))\n",
        "    else:  # 다중 클래스 분류\n",
        "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, logits, from_logits=True))\n",
        "    return loss\n",
        "\n",
        "# Backward propagation\n",
        "@tf.function\n",
        "def backward(model, x, y, optimizer, num_of_class):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x)\n",
        "        loss = cross_entropy_loss(logits, y, num_of_class)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# Accuracy function\n",
        "@tf.function\n",
        "def accuracy(predY, y, num_of_class):\n",
        "    y = tf.cast(y, tf.float32)  # 라벨을 float32로 캐스팅\n",
        "    if num_of_class == 2:  # 이진 분류\n",
        "        predY = tf.round(predY)  # sigmoid 출력에서 예측 클래스 계산\n",
        "        correct_predictions = tf.equal(predY, y)\n",
        "    else:  # 다중 클래스 분류\n",
        "        correct_predictions = tf.equal(tf.argmax(predY, 1), tf.argmax(y, 1))\n",
        "    acc = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "    return acc\n",
        "\n",
        "# Training function\n",
        "def train_learning(epochs, frequence, train_dataset, model, test_dataset, optimizer, num_of_class):\n",
        "    y_loss = []\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for batch, (bx, by) in enumerate(train_dataset):\n",
        "            loss = backward(model, bx, by, optimizer, num_of_class)\n",
        "            y_loss.append(loss)\n",
        "        if epoch % frequence == 0:\n",
        "            acc_list = []\n",
        "            for test_batch, (tx, ty) in enumerate(test_dataset):\n",
        "                predY = model(tx)\n",
        "                acc = accuracy(predY, ty, num_of_class)\n",
        "                acc_list.append(acc)\n",
        "            print(f\"Epoch: {epoch+1} ===> Loss: {loss.numpy()}, accuracy: {np.mean([a.numpy() for a in acc_list])}\")\n",
        "    return y_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E_pJ2YKrvMlF"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing function\n",
        "def get_train_test_data(df_train, df_test, num_of_class=2):\n",
        "    train_x = df_train[\"text\"].values\n",
        "    train_y = df_train[\"labels\"].values\n",
        "    test_x = df_test[\"text\"].values\n",
        "    test_y = df_test[\"labels\"].values\n",
        "\n",
        "    train_x = np.array(train_x)\n",
        "    if num_of_class > 2:\n",
        "        train_y = tf.keras.utils.to_categorical(train_y, num_of_class)\n",
        "        test_y = tf.keras.utils.to_categorical(test_y, num_of_class)\n",
        "    else:\n",
        "        train_y = np.expand_dims(train_y, axis=-1)  # 이진 분류에서 형상 맞추기\n",
        "        test_y = np.expand_dims(test_y, axis=-1)\n",
        "\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "# Evaluate the model on test data (custom implementation)\n",
        "def evaluate_model(model, test_dataset, num_of_class):\n",
        "    acc_list = []\n",
        "    loss_list = []\n",
        "    for test_batch, (tx, ty) in enumerate(test_dataset):\n",
        "        predY = model(tx)\n",
        "        loss = cross_entropy_loss(predY, ty, num_of_class)\n",
        "        acc = accuracy(predY, ty, num_of_class)\n",
        "        loss_list.append(loss.numpy())\n",
        "        acc_list.append(acc.numpy())\n",
        "    return np.mean(loss_list), np.mean(acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkAjddKQgRwG",
        "outputId": "be4d5b61-dde2-4870-afa9-4e2107d67a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아 더빙.. 진짜 짜증나네요 목소리' '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나'\n",
            " '너무재밓었다그래서보는것을추천한다' ... '이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?'\n",
            " '청춘 영화의 최고봉.방황과 우울했던 날들의 자화상' '한국 영화 최초로 수간하는 내용이 담긴 영화']\n"
          ]
        }
      ],
      "source": [
        "train_text = pd.DataFrame(corpus.train.texts, columns=['text'])\n",
        "train_label = pd.DataFrame(corpus.train.labels, columns=['labels'])\n",
        "train = pd.concat([train_text, train_label], axis=1)\n",
        "# print(train)\n",
        "\n",
        "\n",
        "test_text = pd.DataFrame(corpus.test.texts, columns=['text'])\n",
        "test_label = pd.DataFrame(corpus.test.labels, columns=['labels'])\n",
        "test = pd.concat([test_text, test_label], axis=1)\n",
        "# print(train)\n",
        "cnt_labels =2\n",
        "\n",
        "X_train, y_train, X_test, y_test = get_train_test_data(train, test, cnt_labels)\n",
        "print(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sl3BFHkxnFli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc52bd9d-ef5c-43be-d044-c380ac224cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "max_features = 5000\n",
        "sequence_length = 10\n",
        "embedding_dim = 1024  # 의미 입려값을 전환값\n",
        "hidden_size = 512\n",
        "num_of_class = 2  # 이진 분류: 2, 다중 클래스 분류: 그 이상\n",
        "batch_size = 32\n",
        "\n",
        "# Vectorization\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode='int', output_sequence_length=sequence_length)\n",
        "vectorize_layer.adapt(tf.constant(X_train))\n",
        "vocab_set = vectorize_layer.get_vocabulary()\n",
        "vocab_size = len(vocab_set)\n",
        "print(vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "# Vectorize the input data\n",
        "X_Train_vectorized = vectorize_layer(tf.constant(X_train))\n",
        "X_Test_vectorized = vectorize_layer(tf.constant(X_test))\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_Train_vectorized, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_Test_vectorized, y_test)).batch(batch_size)\n",
        "\n",
        "# Hyperparameters\n",
        "dropout_rate = 0.5  # Dropout 비율\n",
        "\n",
        "# Model initialization with dropout\n",
        "model = LSTMModel(sequence_length, vocab_size, embedding_dim, hidden_size, num_of_class, dropout_rate)\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 1e-4\n",
        "optimizer = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Model training\n",
        "epochs = 100\n",
        "frequence = 10\n",
        "# y_loss = train_learning(epochs, frequence, train_dataset, model, test_dataset, optimizer, num_of_class)\n",
        "\n",
        "\n",
        "# Use custom evaluation function\n",
        "# test_loss, test_accuracy = evaluate_model(model, test_dataset, num_of_class)\n",
        "# print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# path = \"/content/drive/MyDrive/세종교육/LSTM/model_version/\"\n",
        "# model.save(f\"{path}Movie_LSTM_model_tf_{tf.__version__}_v0\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_set = vectorize_layer.get_vocabulary()\n",
        "print(vocab_set)"
      ],
      "metadata": {
        "id": "0HoxATjR9C-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words= ['더', '제', '하게', '가','데', '세', 'is','는', '에서','의해','인', '나고']"
      ],
      "metadata": {
        "id": "PsS4g9RK9DQN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AJuKwbrRlxt76jXqkSpOlTyt9daxioAt",
      "authorship_tag": "ABX9TyMMY0rGIGI5U/XSxFDyRJu5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}