{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1J7C9f157cCxKwfp1XgF8RVcXbioTI_Cy",
      "authorship_tag": "ABX9TyM6BEAPNehhEih4ELHM/cVk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/welcomeglory/Python/blob/master/cat_dog_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozLsEDtq9y9W",
        "outputId": "5f2744c5-3e9a-4a46-882a-ab1a3db6cd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.15.0\n",
        "import os\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# path = \"/content/drive/MyDrive/세종교육/CNN/model_version/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCNN(tf.keras.Model):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ImageCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='same', activation='relu')\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)\n",
        "\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu')\n",
        "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)\n",
        "\n",
        "        self.conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=5, strides=1, padding='same', activation='relu')\n",
        "        self.conv4 = tf.keras.layers.Conv2D(filters=128, kernel_size=5, strides=1, padding='same', activation='relu')\n",
        "\n",
        "        self.flat = tf.keras.layers.Flatten()\n",
        "        self.hid1 = tf.keras.layers.Dense(1024, activation='relu')\n",
        "        self.drop1 = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(num_classes, activation=None)\n",
        "\n",
        "    def call(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        pool1 = self.pool1(conv1)\n",
        "        conv2 = self.conv2(pool1)\n",
        "        pool2 = self.pool2(conv2)\n",
        "        conv3 = self.conv3(pool2)\n",
        "        conv4 = self.conv4(conv3)\n",
        "        flat = self.flat(conv4)\n",
        "        hid = self.hid1(flat)\n",
        "        drop = self.drop1(hid)\n",
        "        logits = self.output_layer(drop)\n",
        "        return logits\n",
        "\n",
        "# 옵티마이저 생성\n",
        "optimizer = tf.keras.optimizers.legacy.Adam()\n",
        "\n",
        "@tf.function\n",
        "def backward(model, x, y, train_summary):\n",
        "    with tf.GradientTape() as grad:\n",
        "        logits = model(x)\n",
        "        loss = cross_entropy_loss(logits, y)\n",
        "\n",
        "    grads = grad.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    with train_summary.as_default():\n",
        "        tf.summary.scalar('loss', loss, step=optimizer.iterations)\n",
        "        tf.summary.image('training image', x, max_outputs=9, step=optimizer.iterations)\n",
        "\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def cross_entropy_loss(logits, y):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "\n",
        "@tf.function\n",
        "def accuracy(pred, y):\n",
        "    correction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "    return tf.reduce_mean(tf.cast(correction, tf.float32))"
      ],
      "metadata": {
        "id": "eRbWv3uMdxvz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(testDataIter, model):\n",
        "    accuracies = []\n",
        "    for batch in testDataIter:\n",
        "        b_x, b_y = batch\n",
        "        pred = model(b_x)\n",
        "        acc = accuracy(pred, b_y)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    avg_acc = tf.reduce_mean(accuracies)\n",
        "    print(f\"Final Test Accuracy: {avg_acc}\")\n",
        "    return avg_acc\n",
        "\n",
        "def train_learning(epochs, frq_print, trainDataIter, testDataIter, model, optimizer, path=''):\n",
        "    # 체크포인트 및 체크포인트 관리자 설정\n",
        "    ckpt = tf.train.Checkpoint(step=tf.Variable(0), model=model, optimizer=optimizer)\n",
        "    ckpt_mgr = tf.train.CheckpointManager(ckpt, directory=f\"{path}checkPoint/\", max_to_keep=5)\n",
        "    latest_ckpt = ckpt_mgr.latest_checkpoint\n",
        "    train_summary = tf.summary.create_file_writer(f\"{path}tensorboard/train\")\n",
        "\n",
        "    # 이전 체크포인트가 있으면 복원하고 옵티마이저 초기화\n",
        "    if latest_ckpt:\n",
        "        ckpt.restore(latest_ckpt)\n",
        "        print(f\"Restored from {latest_ckpt}\")\n",
        "\n",
        "        # 더미 입력으로 모델 호출 (옵티마이저가 변수를 인식하도록)\n",
        "        try:\n",
        "            b_x, b_y = next(trainDataIter)\n",
        "        except StopIteration:\n",
        "            trainDataIter = iter(train_dataset)  # train_dataset을 이터레이터로 재초기화\n",
        "            b_x, b_y = next(trainDataIter)\n",
        "\n",
        "        _ = model(b_x)  # 실제 데이터를 사용하여 모델 호출\n",
        "\n",
        "        evaluate_model(testDataIter, model)\n",
        "\n",
        "    # 초기화된 loss 리스트\n",
        "    lossList = []\n",
        "\n",
        "    # 훈련 루프\n",
        "    for epoch in range(int(ckpt.step), epochs):\n",
        "        try:\n",
        "            b_x, b_y = next(trainDataIter)\n",
        "        except StopIteration:\n",
        "            # 데이터셋 이터레이터가 끝난 경우, 이터레이터를 다시 초기화\n",
        "            trainDataIter = iter(train_dataset)  # train_dataset을 이터레이터로 재초기화\n",
        "            b_x, b_y = next(trainDataIter)\n",
        "\n",
        "        # 백워드 패스 및 손실 계산\n",
        "        ls = backward(model, b_x, b_y, train_summary)\n",
        "        lossList.append(ls)\n",
        "\n",
        "        # 주기적으로 체크포인트 저장 및 모델 성능 출력\n",
        "        if epoch % frq_print == 0:\n",
        "            ckpt_mgr.save(checkpoint_number=ckpt.step)\n",
        "            pred = model(b_x)\n",
        "            acc = accuracy(pred, b_y)\n",
        "            print(f\"Epoch: {epoch + 1} ====> Loss: {ls}, Accuracy: {acc}\")\n",
        "\n",
        "        # 에포크 수 증가\n",
        "        ckpt.step.assign_add(1)\n",
        "\n",
        "    # 최종 평가\n",
        "    evaluate_model(testDataIter, model)\n",
        "\n",
        "    return lossList"
      ],
      "metadata": {
        "id": "azf0GCMidyQR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(paths, labels, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    dataset = dataset.map(lambda x, y: (load_and_preprocess_image(x), tf.one_hot(y, depth=2)))\n",
        "    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# 이미지를 숫자화\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    image = tf.cast(image, tf.float32) / 255.0 #normalization\n",
        "    return image"
      ],
      "metadata": {
        "id": "RsmjkoU9-VKH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/세종교육/CNN/cat_dog\"\n",
        "files = os.listdir(path)\n",
        "files = [path + f\"/{file_name}\" for file_name in files]\n",
        "cats = [(file_name, 0) for file_name in files if \"cat.\" in file_name]\n",
        "dogs = [(file_name, 1) for file_name in files if \"dog.\" in file_name]\n",
        "\n",
        "print(cats[0])\n",
        "\n",
        "train_dogs = dogs[:int(len(dogs)*0.7)] # 다운사이징\n",
        "test_dogs = dogs[int(len(dogs)*0.7):]\n",
        "train_cats = cats[:int(len(cats)*0.7)] # 다운사이징\n",
        "test_cats = cats[int(len(cats)*0.7):]\n",
        "\n",
        "train = train_dogs + train_cats\n",
        "test = test_dogs + test_cats\n",
        "\n",
        "print(len(train))\n",
        "print(len(test))\n",
        "\n",
        "\n",
        "trains, train_labels = zip(*train)\n",
        "trains = list(trains)\n",
        "train_labels  = list(train_labels)\n",
        "\n",
        "tests, test_labels = zip(*test)\n",
        "tests = list(tests)\n",
        "test_labels  = list(test_labels)\n",
        "\n",
        "\n",
        "train_dataset = load_dataset(trains, train_labels, batch_size=64)\n",
        "train_dataset_iter = iter(train_dataset)\n",
        "test_dataset = load_dataset(tests, test_labels, batch_size=64)\n",
        "test_dataset_iter = iter(test_dataset)\n",
        "\n",
        "for images, labels in train_dataset_iter:\n",
        "  print(images.shape)\n",
        "  print(labels.shape)\n",
        "  break\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbzznR6hNoSs",
        "outputId": "147b83b7-5c8c-41ae-9d9a-8a75153c6171"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('/content/drive/MyDrive/세종교육/CNN/cat_dog/cat.9163.jpg', 0)\n",
            "17500\n",
            "7500\n",
            "(64, 128, 128, 3)\n",
            "(64, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/세종교육/model_version/\"\n",
        "\n",
        "epochs = 10000 # 훈련횟수\n",
        "frq_print = 100\n",
        "model = ImageCNN()\n",
        "lossList = train_learning(epochs, frq_print, train_dataset_iter, test_dataset_iter, model, optimizer, path = path)"
      ],
      "metadata": {
        "id": "zM8aIWPxhxEU"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}